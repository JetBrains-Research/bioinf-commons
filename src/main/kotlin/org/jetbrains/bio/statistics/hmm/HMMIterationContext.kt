package org.jetbrains.bio.statistics.hmm

import org.jetbrains.bio.dataframe.DataFrame
import org.jetbrains.bio.statistics.model.IterationContext
import org.jetbrains.bio.viktor.F64Array

/**
 * Iteration context for the both frequentist and Bayesian hidden
 * Markov models.
 *
 * @author Sergei Lebedev
 * @since 09/10/14
 */
abstract class HMMIterationContext(
    numStates: Int,
    val logPriorProbabilities: F64Array,
    val logTransitionProbabilities: F64Array,
    df: DataFrame
) :
    IterationContext(numStates, df) {

    private val numObservations = df.rowsNumber

    // Note(lebedev): for Bayesian models all quantities below are expected
    // values under the variational posterior distribution.
    /**
     * Transition probabilities matrix.
     * xi_{ij}(t) = E[I{s_{t - 1} = i} I{s_t = j}]
     */
    val logXiSums: F64Array = F64Array(numStates, numStates)

    /**
     * A column-stochastic matrix where the (i, t)-th element is the posterior probability
     * that the t-th observation was generated by state i.
     * gamma_i(t) = E[I{s_t = i}]
     */
    val logGammas: F64Array = F64Array(numStates, numObservations)

    val logForwardProbabilities: F64Array = F64Array(numObservations, numStates)
    val logBackwardProbabilities: F64Array = F64Array(numObservations, numStates)

    /**
     * A matrix, where the (t, i)-th element is the probability
     * that the t-th observation was generated by state i.
     */
    val logObservationProbabilities: F64Array = F64Array(numObservations, numStates)  // filled.

    /**
     * Computes expectations of the latent indicator variables.
     *
     *     \gamma_i(t) = E[I{s_t = i}]
     *     \xi_{ij}(t) = E[I{s_{t - 1} = i} I{s_t = j}]
     */
    override fun expect() {
        HMMInternals.logForward(
            df, numStates, logPriorProbabilities,
            logTransitionProbabilities,
            logObservationProbabilities,
            logForwardProbabilities
        )
        HMMInternals.logBackward(
            df, numStates, logTransitionProbabilities,
            logObservationProbabilities,
            logBackwardProbabilities
        )

        val negativeInfinity = F64Array.full(
            numStates, numStates,
            init = Double.NEGATIVE_INFINITY
        )

        val res = negativeInfinity.copy()
        (1 until df.rowsNumber).forEach { s ->
            val logXit = negativeInfinity.copy()
            for (priorState in 0 until numStates) {
                for (nextState in 0 until numStates) {
                    logXit[priorState, nextState] = logForwardProbabilities[s - 1, priorState] +
                            logTransitionProbabilities[priorState, nextState] +
                            logObservationProbabilities[s, nextState] +
                            logBackwardProbabilities[s, nextState]
                }
            }
            logXit.logRescale()
            res.logAddExpAssign(logXit)
        }
        res.copyTo(logXiSums)

        // TODO(lebedev): we can evaluate 'logGammas' simultaneously with
        // 'logXiSums' in the above loop.
        HMMInternals.evaluate(df, logForwardProbabilities, logBackwardProbabilities, logGammas)
    }

    // This is ad-hoc. Please inline.
    fun calculateLogForwardProbabilities(): F64Array {
        refill()
        HMMInternals.logForward(
            df, numStates, logPriorProbabilities, logTransitionProbabilities,
            logObservationProbabilities, logForwardProbabilities
        )
        return logForwardProbabilities
    }
}

